{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve *Easy21* by TD(&#955;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import draw_heatmap\n",
    "from easy21 import Easy21, HIT, STICK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Easy21()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore policy and stepsize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EpsilonGreedyWithDecay(decay_rate, epsilon0):\n",
    "    def epsilon_greedy_with_decay(qvalues, visits):\n",
    "        epsilon = epsilon0 / (1.0 + decay_rate * sum(visits))\n",
    "        n_actions = len(qvalues)\n",
    "        probs = [epsilon / n_actions] * n_actions\n",
    "        #maxidx = qvalues.index(max(qvalues))\n",
    "        maxidx = np.argmax(qvalues)\n",
    "        probs[maxidx] += (1 - epsilon)\n",
    "        return probs\n",
    "    return epsilon_greedy_with_decay\n",
    "\n",
    "explore_policy = EpsilonGreedyWithDecay(0.01, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StepSizeWithDecay(decay_rate, alpha0):\n",
    "    def step_size_with_decay(visit, *args, **kwargs):\n",
    "        return alpha0 / (1.0 + decay_rate * visit)\n",
    "    return step_size_with_decay\n",
    "\n",
    "stepsize = StepSizeWithDecay(1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa(&#955;) algorithm\n",
    "\n",
    "1. Initialize Q function.\n",
    "2. Set $t=0$ and initialize $s_t$.\n",
    "3. Set $e(s, a)=0$ for all states and action $s, a$.\n",
    "4. Choose action $a_t$ by exploration policy to $s_t$.\n",
    "5. Take $a_t$ and observe $r_{t+1}, s_{t+1}$.\n",
    "6. Choose action $a_{t+1}$ by exploration policy to $s_{t+1}$.\n",
    "7. Update $e$ by\n",
    "$$\n",
    "e(s) \\leftarrow \\begin{cases}\n",
    "  \\gamma\\lambda e(s, a) + 1  && \\text{if } s = s_t \\text{ and } a = a_t\\\\\n",
    "  \\gamma\\lambda e(s, a)      && \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "8. For each $(s, a)$, update Q function by\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot  e(s, a) \\left( r_{t+1} + \\gamma Q \\left( s_{t+1}, a_{t+1} \\right) - Q \\left( s_t, a_t \\right) \\right)\n",
    "$$\n",
    "9. If $s_{t+1}$ is not terminal, set $t \\leftarrow t+1$ and go to 5.\n",
    "10. Finish if maximum iteration has reached, otherwise go to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks\n",
    "\n",
    "* $e(s, a)$ is called the *eligibility trace* for state-action pairs. It tracks the contribution of $(s, a)$ to the current value.\n",
    "* The Q function is update not only for the current observation, but for all state-actions every period. In the implementation, we do this by vectorized operations using `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda_episode(qs, ns, env, policy, stepsize, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Run an episode for Sarsa(lambda) control.\n",
    "    \n",
    "    qs and ns are array of shape (s1, s2, ..., a). Where s1, s2,... are state variables and a is action.\n",
    "    We assume that state is a tuple of nonnegative integers, and action is an nonnegative integer, and\n",
    "    use them as indices as-is.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    es = np.zeros_like(qs)\n",
    "    \n",
    "    prob = policy(qs[s], ns[s])\n",
    "    a = random.choices(range(env.num_actions), prob)[0]\n",
    "    while True:\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        alpha = stepsize(ns[(*s, a)])\n",
    "        ns[(*s, a)] += 1.0\n",
    "        \n",
    "        es *= (gamma*lambda_)\n",
    "        es[(*s, a)] += 1.0\n",
    "        if done:\n",
    "            delta = r - qs[(*s, a)]\n",
    "            qs += alpha * es * delta\n",
    "            break\n",
    "        else:\n",
    "            prob = policy(qs[s1], ns[s1])\n",
    "            a1 = random.choices(range(env.num_actions), prob)[0]\n",
    "            delta = r + gamma * qs[(*s1, a1)] - qs[(*s, a)]\n",
    "            qs += alpha * es * delta\n",
    "            s, a = s1, a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add index 0, which is not used by the algorithm.\n",
    "# By doing so states match the indices exactly.\n",
    "qs = np.zeros((22, 11, 2))\n",
    "ns = np.zeros((22, 11, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53efa23154e04c20a2707c4662f6c313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(10**6)):\n",
    "    sarsa_lambda_episode(qs, ns, env, explore_policy, stepsize, 1.0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_state_value(qs):\n",
    "    return np.max(qs, axis=-1)\n",
    "\n",
    "def to_policy(qs):\n",
    "    return np.argmax(qs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = to_state_value(qs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "draw_heatmap(V[1:, 1:], ax=ax, title=\"State Value\")\n",
    "fig.tight_layout()\n",
    "\n",
    "q_diff = qs[:, :, 1] - qs[:, :, 0]\n",
    "policy = to_policy(qs)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 6))\n",
    "draw_heatmap(q_diff[1:, 1:], ax=ax1, title=\"Q(hit) - Q(stick)\")\n",
    "draw_heatmap(policy[1:, 1:], ax=ax2, title=\"Policy(Hit = 1, Stick = 0)\", fmt=\"d\")\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to the DP result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.load(\"result/dp.npz\")\n",
    "Q_dp = f[\"Q\"]\n",
    "Q_dp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = {}\n",
    "for lambda_ in np.arange(0.0, 1.1, 0.1):\n",
    "    lambda_ = np.round(lambda_, decimals=1)  # to avoid small deviation like 0.30000000000000004\n",
    "    clear_output(wait=True)\n",
    "    print(\"lambda =\", lambda_)\n",
    "    qs = np.zeros((22, 11, 2))\n",
    "    ns = np.zeros((22, 11, 2))\n",
    "    mae = []\n",
    "    for i in tqdm(range(10**7)):\n",
    "        sarsa_lambda_episode(qs, ns, env, explore_policy, stepsize, 1.0, lambda_)\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            mae.append(np.mean(np.abs(qs[1:, 1:] - Q_dp)))\n",
    "            print(\"\\rIter #%d, MAE = %.6f\" % (i+1, mae[-1]), end=\"\")\n",
    "    maes[lambda_] = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df = pd.DataFrame(maes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "mae_df.plot(ax=ax)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Mean absolute error\")\n",
    "ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = np.linspace(0, len(mae_df)-1, num=6, dtype=int)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 8), sharex=True, sharey=False)\n",
    "for i, j in itertools.product(range(3), range(2)):\n",
    "    k = iters[2*i + j]\n",
    "    it = 1000 * (k + 1)\n",
    "    ax = axes[i][j]\n",
    "    ax.plot(mae_df.iloc[k], marker=\"o\")\n",
    "    ax.set_xlabel(\"lambda\")\n",
    "    ax.set_ylabel(\"Mean absolute error\")\n",
    "    ax.set_title(\"Iter = %d\" % it)\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy TD(&#955;)\n",
    "\n",
    "We will implement two variants of off-policy TD(&#955;) algorithms, Watkin's and Na&#239;ve Q(&#955;).\n",
    "When extending Q-learning with eligibility traces, consideration taken is the treatment of exploratory actions.\n",
    "* **Watkin's Q(&#955;)**: Zero-out all eligibility traces when non-greedy action is taken.\n",
    "* **Na&#239;ve Q(&#955;)**: Keep all traces as for Sarasa.\n",
    "\n",
    "## Watkins's Q(&#955;)\n",
    "\n",
    "1. Initialize Q function.\n",
    "2. Set $t=0$ and initialize $s_t$.\n",
    "3. Set $e(s, a)=0$ for all states and actions $s, a$.\n",
    "4. Choose action $a_t$ by exploration policy to $s_t$.\n",
    "5. Take $a_t$ and observe $r_{t+1}, s_{t+1}$.\n",
    "6. Choose action $a_{t+1}$ by exploration policy to $s_{t+1}$.\n",
    "7. If $Q(s_{t+1}, a_{t+1}) \\not= \\max_a Q(s_{t+1}, a)$, set $e(s, a) = 0$ for all $s, a$.\n",
    "8. Update $e$ by\n",
    "$$\n",
    "e(s) \\leftarrow \\begin{cases}\n",
    "  \\gamma\\lambda e(s, a) + 1  && \\text{if } s = s_t \\text{ and } a = a_t\\\\\n",
    "  \\gamma\\lambda e(s, a)      && \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "9. For each $(s, a)$, update Q function by\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot  e(s, a) \\left( r_{t+1} + \\gamma \\max_a Q \\left( s_{t+1}, a \\right) - Q \\left( s_t, a_t \\right) \\right)\n",
    "$$\n",
    "10. If $s_{t+1}$ is not terminal, set $t \\leftarrow t+1$ and go to 5.\n",
    "10. Finish if maximum iteration has reached, otherwise go to 2.\n",
    "\n",
    "\n",
    "## Na&#239;ve Q(&#955;)\n",
    "\n",
    "1. Initialize Q function.\n",
    "2. Set $t=0$ and initialize $s_t$.\n",
    "3. Set $e(s, a)=0$ for all states and actions $s, a$.\n",
    "4. Choose action $a_t$ by exploration policy to $s_t$.\n",
    "5. Take $a_t$ and observe $r_{t+1}, s_{t+1}$.\n",
    "6. Update $e$ by\n",
    "$$\n",
    "e(s) \\leftarrow \\begin{cases}\n",
    "  \\gamma\\lambda e(s, a) + 1  && \\text{if } s = s_t \\text{ and } a = a_t\\\\\n",
    "  \\gamma\\lambda e(s, a)      && \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "7. For each $(s, a)$, update Q function by\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot  e(s, a) \\left( r_{t+1} + \\gamma \\max_a Q \\left( s_{t+1}, a \\right) - Q \\left( s_t, a_t \\right) \\right)\n",
    "$$\n",
    "8. If $s_{t+1}$ is not terminal, set $t \\leftarrow t+1$ and go to 4.\n",
    "9. Finish if maximum iteration has reached, otherwise go to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_lambda_episode(qs, ns, env, policy, stepsize, gamma, lambda_, watkins=True):\n",
    "    \"\"\"\n",
    "    Run an episode for Q(lambda) control.\n",
    "    \n",
    "    qs and ns are array of shape (s1, s2, ..., a). Where s1, s2,... are state variables and a is action.\n",
    "    We assume that state is a tuple of nonnegative integers, and action is an nonnegative integer, and\n",
    "    use them as indices as-is.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    es = np.zeros_like(qs)\n",
    "    \n",
    "    prob = policy(qs[s], ns[s])\n",
    "    while True:\n",
    "        a = random.choices(range(env.num_actions), prob)[0]\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        alpha = stepsize(ns[(*s, a)])\n",
    "        ns[(*s, a)] += 1.0\n",
    "        \n",
    "        if watkins and qs[(*s, a)] != max(qs[s]):\n",
    "            es[:] = 0.0\n",
    "        es[(*s, a)] += 1.0\n",
    "        \n",
    "        if done:\n",
    "            delta = r - qs[(*s, a)]\n",
    "            qs += alpha * es * delta\n",
    "            break\n",
    "        else:\n",
    "            prob = policy(qs[s1], ns[s1])\n",
    "            delta = r + gamma * max(qs[s1]) - qs[(*s, a)]\n",
    "            qs += alpha * es * delta\n",
    "            es *= (gamma*lambda_)            \n",
    "            s = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watkins's Q\n",
    "qs = np.zeros((22, 11, 2))\n",
    "ns = np.zeros((22, 11, 2))\n",
    "\n",
    "for i in tqdm(range(10**6)):\n",
    "    q_lambda_episode(qs, ns, env, explore_policy, stepsize, 1.0, 0.5, watkins=True)\n",
    "\n",
    "\n",
    "V = to_state_value(qs)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "draw_heatmap(V[1:, 1:], ax=ax, title=\"State Value\")\n",
    "fig.tight_layout()\n",
    "\n",
    "q_diff = qs[:, :, 1] - qs[:, :, 0]\n",
    "policy = to_policy(qs)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 6))\n",
    "draw_heatmap(q_diff[1:, 1:], ax=ax1, title=\"Q(hit) - Q(stick)\")\n",
    "draw_heatmap(policy[1:, 1:], ax=ax2, title=\"Policy(Hit = 1, Stick = 0)\", fmt=\"d\")\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Q\n",
    "qs = np.zeros((22, 11, 2))\n",
    "ns = np.zeros((22, 11, 2))\n",
    "\n",
    "for i in tqdm(range(10**6)):\n",
    "    q_lambda_episode(qs, ns, env, explore_policy, stepsize, 1.0, 0.5, watkins=False)\n",
    "\n",
    "\n",
    "V = to_state_value(qs)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "draw_heatmap(V[1:, 1:], ax=ax, title=\"State Value\")\n",
    "fig.tight_layout()\n",
    "\n",
    "q_diff = qs[:, :, 1] - qs[:, :, 0]\n",
    "policy = to_policy(qs)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 6))\n",
    "draw_heatmap(q_diff[1:, 1:], ax=ax1, title=\"Q(hit) - Q(stick)\")\n",
    "draw_heatmap(policy[1:, 1:], ax=ax2, title=\"Policy(Hit = 1, Stick = 0)\", fmt=\"d\")\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remark: Both Q(&#955;) methods seem to be working well, although their convergence property is theoretically unknown.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to the DP result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = {}\n",
    "for lambda_, watkins in itertools.product(np.arange(0.0, 1.1, 0.1), [True, False]):\n",
    "    lambda_ = np.round(lambda_, decimals=1)  # to avoid small deviation like 0.30000000000000004\n",
    "    clear_output(wait=True)\n",
    "    print(\"lambda, watkins =\", lambda_, watkins)\n",
    "    qs = np.zeros((22, 11, 2))\n",
    "    ns = np.zeros((22, 11, 2))\n",
    "    mae = []\n",
    "    for i in tqdm(range(10**7)):\n",
    "        sarsa_lambda_episode(qs, ns, env, explore_policy, stepsize, 1.0, lambda_)\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            mae.append(np.mean(np.abs(qs[1:, 1:] - Q_dp)))\n",
    "            print(\"\\rIter #%d, MAE = %.6f\" % (i+1, mae[-1]), end=\"\")\n",
    "    maes[(watkins, lambda_)] = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df = pd.DataFrame(maes)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "mae_df[True].plot(ax=ax1)\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.set_ylabel(\"Mean absolute error\")\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.set_title(\"Watkins's Q($\\lambda$)\")\n",
    "\n",
    "mae_df[False].plot(ax=ax2)\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "ax2.set_ylabel(\"Mean absolute error\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_title(\"Naive Q($\\lambda$)\")\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = np.linspace(0, len(mae_df)-1, num=6, dtype=int)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 8), sharex=True, sharey=False)\n",
    "for i, j in itertools.product(range(3), range(2)):\n",
    "    k = iters[2*i + j]\n",
    "    it = 1000 * (k + 1)\n",
    "    ax = axes[i][j]\n",
    "    ax.plot(mae_df[True].iloc[k], marker=\"o\", label=\"Watkins Q($\\lambda$)\")\n",
    "    ax.plot(mae_df[False].iloc[k], marker=\"o\", label=\"Naive Q($\\lambda$)\")\n",
    "    ax.set_xlabel(\"lambda\")\n",
    "    ax.set_ylabel(\"Mean absolute error\")\n",
    "    ax.set_title(\"Iter = %d\" % it)\n",
    "    ax.legend()\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
